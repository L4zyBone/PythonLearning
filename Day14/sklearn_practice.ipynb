{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:06:30.461130Z",
     "start_time": "2025-01-09T13:06:28.307121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "id": "9d8ed76b1cb74cd9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1、完成DictVectorizer，CountVectorizer, TfidfVectorizer的练习，并理解其原理",
   "id": "c94eda68aebda2fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:20:03.334903Z",
     "start_time": "2025-01-09T13:20:03.328025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dictvec():\n",
    "    \"\"\"\n",
    "    字典数据抽取\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 实例化\n",
    "    # sparse改为True,输出的是每个不为零位置的坐标，稀疏矩阵可以节省存储空间\n",
    "    #矩阵中存在大量的0，sparse存储只记录非零位置，节省空间的作用\n",
    "    #Vectorizer中文含义是矢量器的含义\n",
    "    dict1 = DictVectorizer(sparse=True)  # sparse为False时输出完整矩阵\n",
    "\n",
    "    #每个样本都是一个字典，有三个样本\n",
    "    # 调用fit_transform\n",
    "    data = dict1.fit_transform([{'city': '北京', 'temperature': 100},\n",
    "                                {'city': '上海', 'temperature': 60},\n",
    "                                {'city': '深圳', 'temperature': 30}])\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    # 字典中的一些类别数据，分别进行转换成特征\n",
    "    print(dict1.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(dict1.inverse_transform(data))  #去看每个特征代表的含义，逆转回去\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "dictvec()"
   ],
   "id": "2f8606b1621e4e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 6 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n",
      "--------------------------------------------------\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "--------------------------------------------------\n",
      "[{'city=北京': np.float64(1.0), 'temperature': np.float64(100.0)}, {'city=上海': np.float64(1.0), 'temperature': np.float64(60.0)}, {'city=深圳': np.float64(1.0), 'temperature': np.float64(30.0)}]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:34:59.148714Z",
     "start_time": "2025-01-09T13:34:59.133692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def couvec():\n",
    "    # 实例化CountVectorizer\n",
    "    # max_df, min_df整数：指每个词的所有文档词频数不小于最小值，出现该词的文档数目小于等于max_df\n",
    "    # max_df, min_df小数(0-1之间的）：某个词的出现的次数／所有文档数量\n",
    "    # min_df=2\n",
    "    # 默认会去除单个字母的单词，默认认为这个词对整个样本没有影响,认为其没有语义\n",
    "    vector = CountVectorizer(min_df=2)\n",
    "\n",
    "    # 调用fit_transform输入并转换数据\n",
    "\n",
    "    res = vector.fit_transform(\n",
    "        [\"life is  short,i like python life\",\n",
    "         \"life is too long,i dislike python\",\n",
    "         \"life is short\"])\n",
    "\n",
    "    # 打印结果,把每个词都分离了\n",
    "    print(vector.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(res)\n",
    "    print('-' * 50)\n",
    "    print(type(res))\n",
    "    # 对照feature_names，标记每个词出现的次数\n",
    "    print('-' * 50)\n",
    "    print(res.toarray())  #稀疏矩阵转换为数组\n",
    "    print('-' * 50)\n",
    "    #拿每个样本里的特征进行显示\n",
    "    print(vector.inverse_transform(res))\n",
    "\n",
    "\n",
    "couvec()"
   ],
   "id": "9451c1fc6c67ddc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'life' 'python' 'short']\n",
      "--------------------------------------------------\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[1 2 1 1]\n",
      " [1 1 1 0]\n",
      " [1 1 0 1]]\n",
      "--------------------------------------------------\n",
      "[array(['life', 'is', 'short', 'python'], dtype='<U6'), array(['life', 'is', 'python'], dtype='<U6'), array(['life', 'is', 'short'], dtype='<U6')]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:43:38.650981Z",
     "start_time": "2025-01-09T13:43:37.856833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cutword():\n",
    "    \"\"\"\n",
    "    通过jieba对中文进行分词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "\n",
    "    con3 = jieba.cut(\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "\n",
    "    # 转换成列表\n",
    "    print(type(con1))\n",
    "    print('-' * 50)\n",
    "    # 把生成器转换成列表\n",
    "    content1 = list(con1)\n",
    "    content2 = list(con2)\n",
    "    content3 = list(con3)\n",
    "    print(content1)\n",
    "    print(content2)\n",
    "    print(content3)\n",
    "    # 把列表转换成字符串,每个词之间用空格隔开\n",
    "    print('-' * 50)\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "\n",
    "    return c1, c2, c3\n",
    "\n",
    "\n",
    "def hanzivec():\n",
    "    \"\"\"\n",
    "    中文特征值化\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()  #jieba分词好的中文文本\n",
    "    print('-' * 50)\n",
    "    print(c1)  #把分词好的文本打印出来\n",
    "    print(c2)\n",
    "    print(c3)\n",
    "    print('-' * 50)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    data = cv.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print(cv.get_feature_names_out())  #把处理好后的特征名称打印出来\n",
    "\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# cutword()\n",
    "hanzivec()"
   ],
   "id": "351088f9481366d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\Bao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.778 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。\n",
      "如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "[[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:44:00.916414Z",
     "start_time": "2025-01-09T13:44:00.902207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 规范{'l1'，'l2'}，默认='l2'\n",
    "# 每个输出行都有单位范数，或者：\n",
    "#\n",
    "# 'l2'：向量元素的平方和为 1。当应用 l2 范数时，两个向量之间的余弦相似度是它们的点积。\n",
    "#\n",
    "# 'l1'：向量元素的绝对值之和为 1。参见preprocessing.normalize。\n",
    "\n",
    "# smooth_idf布尔值，默认 = True\n",
    "# 通过在文档频率上加一来平滑 idf 权重，就好像看到一个额外的文档包含集合中的每个术语恰好一次。防止零分裂。\n",
    "# 比如训练集中有某个词，测试集中没有，就是生僻词，就会造成n(x)分母为零，log(n/n(x)),从而出现零分裂\n",
    "\n",
    "def tfidfvec():\n",
    "    \"\"\"\n",
    "    中文特征值化,计算tfidf值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()\n",
    "\n",
    "    print(c1, c2, c3)\n",
    "    # print(type([c1, c2, c3]))\n",
    "    tf = TfidfVectorizer(smooth_idf=True)\n",
    "\n",
    "    data = tf.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print(tf.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(type(data))\n",
    "    print('-' * 50)\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "tfidfvec()"
   ],
   "id": "eabcb4555875e976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "--------------------------------------------------\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。 我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。 如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2、MinMaxScaler, StandardScaler练习并理解原理",
   "id": "44c1d64ce00763d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T13:50:57.931791Z",
     "start_time": "2025-01-09T13:50:57.922583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mm():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    :return: NOne\n",
    "    \"\"\"\n",
    "    # 归一化缺点 容易受极值的影响\n",
    "    #feature_range代表特征值范围，一般设置为(0,1),或者(-1,1),默认是(0,1)\n",
    "    mm = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "\n",
    "    print(data) # 该列的（x-最小值）/（最大值-最小值）\n",
    "    print('-' * 50)\n",
    "    out = mm.transform([[1, 2, 3, 4], [6, 5, 8, 7]])\n",
    "    print(out)\n",
    "    return None\n",
    "    #transform和fit_transform不同是，transform用于测试集，而且不会重新找最小值和最大值\n",
    "\n",
    "\n",
    "mm()"
   ],
   "id": "bdcbffc7b8ed8e7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n",
      "--------------------------------------------------\n",
      "[[-1.96666667  0.         -1.4        -6.        ]\n",
      " [-1.8         1.5        -0.4        -5.5       ]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:00:14.275580Z",
     "start_time": "2025-01-09T14:00:14.264459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stand():\n",
    "    \"\"\"\n",
    "    标准化缩放，不是标准正太分布，只均值为0，方差为1的分布\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    std = StandardScaler()\n",
    "\n",
    "    data = std.fit_transform([[1., -1., 3.],\n",
    "                              [2., 4., 2.],\n",
    "                              [4., 6., -1.]])\n",
    "\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    print(std.mean_)\n",
    "    print('-' * 50)\n",
    "    print(std.var_)  #方差\n",
    "    print(std.n_samples_seen_)  # 样本数\n",
    "    return data\n",
    "\n",
    "\n",
    "data = stand()"
   ],
   "id": "729f58282f8262ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n",
      "--------------------------------------------------\n",
      "[2.33333333 3.         1.33333333]\n",
      "--------------------------------------------------\n",
      "[1.55555556 8.66666667 2.88888889]\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3、完成SimpleImputer的练习",
   "id": "cfdc8692fd950b91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:22:32.741552Z",
     "start_time": "2025-01-09T14:22:32.728996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#下面是填补，针对删除，可以用pd和np\n",
    "def im():\n",
    "    \"\"\"\n",
    "    缺失值处理\n",
    "    :return:None\n",
    "    \"\"\"\n",
    "    # NaN, nan,缺失值必须是这种形式，如果是？号(或者其他符号)，就要replace换成这种\n",
    "    #mean, median, most_frequent(众数), constant\n",
    "    im = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    data = im.fit_transform([[1, 2], [np.nan, 3], [7, 6], [3, 2]]) # 将nan填充为均值\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "im()"
   ],
   "id": "b18d0d2619048395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.        ]\n",
      " [3.66666667 3.        ]\n",
      " [7.         6.        ]\n",
      " [3.         2.        ]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4、完成VarianceThreshold，PCA的练习并理解原理",
   "id": "f5808cb79666ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:25:30.776995Z",
     "start_time": "2025-01-09T14:25:30.770773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def var():\n",
    "    \"\"\"\n",
    "    特征选择-删除低方差的特征\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #默认只删除方差为0,threshold是方差阈值，删除比这个值小的那些特征\n",
    "    var = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "    data = var.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "    # 获得剩余的特征的列编号\n",
    "    print('The surport is %s' % var.get_support(True))\n",
    "    return None\n",
    "\n",
    "\n",
    "var()"
   ],
   "id": "809132de15485c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n",
      "--------------------------------------------------\n",
      "The surport is [1 2]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:27:02.859179Z",
     "start_time": "2025-01-09T14:27:02.836566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pca():\n",
    "    \"\"\"\n",
    "    主成分分析进行特征降维\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # n_ components:小数 0~1 90% 业界选择 90~95%\n",
    "\n",
    "    # 当n_components的值为0到1之间的浮点数时，表示我们希望保留的主成分解释的方差比例。方差比例是指 得到输出的每一列的方差值和除以原有数据方差之和。\n",
    "    # 具体而言，n_components=0.9表示我们希望选择足够的主成分，以使它们解释数据方差的90%。\n",
    "\n",
    "    # n_components如果是整数   减少到的特征数量\n",
    "    # 原始数据方差\n",
    "    original_value = np.array([[2, 8, 4, 5],\n",
    "                               [6, 3, 0, 8],\n",
    "                               [5, 4, 9, 1]])\n",
    "    print(np.var(original_value, axis=0).sum())  #最初数据每一列的方差，求和\n",
    "    print('-' * 50)\n",
    "    pca = PCA(n_components=0.99)\n",
    "\n",
    "    data = pca.fit_transform(original_value)\n",
    "\n",
    "    print(data)\n",
    "    print(type(data))\n",
    "    #计算data的方差和\n",
    "    print(np.var(data, axis=0).sum())\n",
    "    print('-' * 50)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    # 计算data的方差占总方差的比例\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "pca()"
   ],
   "id": "6c3ec94ebd0f0549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.333333333333336\n",
      "--------------------------------------------------\n",
      "[[-1.28620952e-15  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n",
      "<class 'numpy.ndarray'>\n",
      "29.333333333333332\n",
      "--------------------------------------------------\n",
      "[0.75 0.25]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5、完成load及fetch数据加载，搞清数据集的特征和目标，train_test_split样本切分",
   "id": "6cd1d17204842d82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:35:32.221567Z",
     "start_time": "2025-01-09T14:35:29.229578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_20newsgroups, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "id": "bfe51d8c97d09438",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:36:15.700798Z",
     "start_time": "2025-01-09T14:36:15.682891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#鸢尾花数据集，查看特征，目标，样本量\n",
    "\n",
    "li = load_iris()\n",
    "\n",
    "print(\"获取特征值\")\n",
    "print(type(li.data))\n",
    "print('-' * 50)\n",
    "print(li.data.shape) # 150个样本，4个特征,一般看shape\n",
    "li.data"
   ],
   "id": "88021128a25adde9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "<class 'numpy.ndarray'>\n",
      "--------------------------------------------------\n",
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:37:07.169003Z",
     "start_time": "2025-01-09T14:37:07.163591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"目标值\")\n",
    "print(li.target)\n",
    "print('-' * 50)\n",
    "print(li.DESCR)\n",
    "print('-' * 50)\n",
    "print(li.feature_names)  # 重点,特征名字\n",
    "print('-' * 50)\n",
    "print(li.target_names) # 目标名字"
   ],
   "id": "89bad94de1b42b15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "--------------------------------------------------\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "--------------------------------------------------\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:41:00.501062Z",
     "start_time": "2025-01-09T14:41:00.489170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 注意返回值, 训练集 train  x_train, y_train        测试集  test   x_test, y_test，顺序千万别搞错了\n",
    "# 默认是乱序的,random_state为了确保两次的随机策略一致，就会得到相同的随机数据，往往会带上\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# print(\"训练集特征值和目标值：\", x_train, y_train)\n",
    "print(\"训练集特征值shape\", x_train.shape)\n",
    "print('-'*50)\n",
    "# print(\"测试集特征值和目标值：\", x_test, y_test)\n",
    "print(\"测试集特征值shape\", x_test.shape)"
   ],
   "id": "5b81bb748528f8af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征值shape (112, 4)\n",
      "--------------------------------------------------\n",
      "测试集特征值shape (38, 4)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:43:11.009493Z",
     "start_time": "2025-01-09T14:43:10.765582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#subset代表下载的数据集类型，默认是train，只有训练集\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "# print(news.feature_names)  #这个数据集是没有的，因为没有特征，只有文本数据\n",
    "# print(news.DESCR)\n",
    "print('第一个样本')\n",
    "print(news.data[0])"
   ],
   "id": "9cd536c5fb9fa14c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个样本\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:43:48.801872Z",
     "start_time": "2025-01-09T14:43:48.796861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('特征类型')\n",
    "print(type(news.data))\n",
    "print('-' * 50)\n",
    "print(news.target[0:15])\n",
    "from pprint import pprint\n",
    "pprint(list(news.target_names))"
   ],
   "id": "7c4d68368a123f15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征类型\n",
      "<class 'list'>\n",
      "--------------------------------------------------\n",
      "[10  3 17  3  4 12  4 10 10 19 19 11 19 13  0]\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:44:03.792904Z",
     "start_time": "2025-01-09T14:44:03.788033Z"
    }
   },
   "cell_type": "code",
   "source": "len(news.target_names)",
   "id": "8895f80da14e19fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:44:10.171273Z",
     "start_time": "2025-01-09T14:44:10.162239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('-' * 50)\n",
    "print(len(news.data))\n",
    "print('新闻所有的标签')\n",
    "print(news.target)\n",
    "print('-' * 50)\n",
    "print(min(news.target), max(news.target))"
   ],
   "id": "e4d164db0e1f51bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "18846\n",
      "新闻所有的标签\n",
      "[10  3 17 ...  3  1  7]\n",
      "--------------------------------------------------\n",
      "0 19\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:44:26.161203Z",
     "start_time": "2025-01-09T14:44:26.146204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "house=fetch_california_housing(data_home='data')\n",
    "print(\"获取特征值\")\n",
    "print(house.data[0])  #第一个样本特征值\n",
    "print('样本的形状')\n",
    "print(house.data.shape)\n",
    "print('-' * 50)"
   ],
   "id": "c9b5c687a7893b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "样本的形状\n",
      "(20640, 8)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:02.379643Z",
     "start_time": "2025-01-09T14:45:02.374119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"目标值\")\n",
    "print(house.target[0:10])\n",
    "print('-' * 50)\n",
    "print(house.DESCR)\n",
    "print('-' * 50)\n",
    "print(house.feature_names)\n",
    "print('-' * 50)"
   ],
   "id": "58a247cd40e8c964",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值\n",
      "[4.526 3.585 3.521 3.413 3.422 2.697 2.992 2.414 2.267 2.611]\n",
      "--------------------------------------------------\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "--------------------------------------------------\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
